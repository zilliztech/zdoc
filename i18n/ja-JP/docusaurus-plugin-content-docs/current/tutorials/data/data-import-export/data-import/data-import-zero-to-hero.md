---
title: "ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒãƒ³ã‚ºã‚ªãƒ³ | Cloud"
slug: /data-import-zero-to-hero
sidebar_label: "ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒãƒ³ã‚ºã‚ªãƒ³"
beta: FALSE
notebook: FALSE
description: "Zilliz Cloudã¸ã®ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã€ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚„åé›†ã®è¨­å®šã‹ã‚‰å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®éç¨‹ã¾ã§ã€è¿…é€Ÿã«é–‹å§‹ã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ã‚¹ã§ã™ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ä»¥ä¸‹ã®ã“ã¨ã‚’å­¦ã³ã¾ã™ | Cloud"
type: origin
token: VY2Iw7ZFLihNwekvTL0c2VNhnRh
sidebar_position: 5
keywords: 
  - zilliz
  - vector database
  - cloud
  - data import
  - milvus
  - Anomaly Detection
  - sentence transformers
  - Recommender systems
  - information retrieval

---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒãƒ³ã‚ºã‚ªãƒ³

Zilliz Cloudã¸ã®ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã€ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚„åé›†ã®è¨­å®šã‹ã‚‰å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®éç¨‹ã¾ã§ã€è¿…é€Ÿã«é–‹å§‹ã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ã‚¹ã§ã™ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ä»¥ä¸‹ã®ã“ã¨ã‚’å­¦ã³ã¾ã™:

- ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨­å®šã™ã‚‹æ–¹æ³•

- BulkWriterã‚’ä½¿ç”¨ã—ã¦ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿**ã‚’**æº–å‚™ã—ã€ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒã‚±ãƒƒãƒˆã«æ›¸ãè¾¼ã‚€æ–¹æ³•

- ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆAPIã‚’å‘¼ã³å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•

## å§‹ã‚ã‚‹å‰ã«{#before-you-start}

ã‚¹ãƒ ãƒ¼ã‚ºãªä½“é¨“ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®è¨­å®šã‚’å®Œäº†ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

### Zilliz Cloudã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—{#set-up-your-zilliz-cloud-cluster}

- ã¾ã ä½œæˆã—ã¦ã„ãªã„å ´åˆã¯ã€[ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆã—](./create-cluster)ã¾ã™ã€‚

- ã“ã‚Œã‚‰ã®è©³ç´°ã‚’åé›†ã—ã¦ãã ã•ã„:**ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**ã€**APIã‚­ãƒ¼**ã€**ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ID**ã€‚

### ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«{#install-dependencies}

ç¾åœ¨ã€Pythonã¾ãŸã¯Javaã§ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–¢é€£APIã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>

<TabItem value='python'>

Python APIã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã€**pymilvus**ã¨**minio**ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã‹ã€æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

```shell
python3 -m pip install --upgrade pymilvus minio
```

</TabItem>

<TabItem value='java'>

- Apache Mavenã®å ´åˆã€**pom. xml**ã®ä¾å­˜é–¢ä¿‚ã«ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„:

```java
<dependency>
  <groupId>io.milvus</groupId>
  <artifactId>milvus-sdk-java</artifactId>
  <version>2.4.8</version>
</dependency>
```

- Gradle/Grailsã®å ´åˆã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```shell
compile 'io.milvus:milvus-sdk-java:2.4.8'
```

</TabItem>

</Tabs>

### ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒã‚±ãƒƒãƒˆã®è¨­å®š{#configure-your-remote-storage-bucket}

- ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã‚’è¨­å®šã™ã‚‹ã«ã¯ã€AWSS 3ã€Google GCSã€ã¾ãŸã¯AzureBlobã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

- ãƒ¡ãƒ¢ã—ã¦ãã ã•ã„

    - **S 3äº’æ›ãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼**ã€**ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚­ãƒ¼**ã€ãŠã‚ˆã³**ãƒã‚±ãƒƒãƒˆå**ã€‚

    - **AccountName**ã€**AccountKey**ã€ãŠã‚ˆã³**ContainerName**Microsoft Azure BLOBã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹ã€‚

    ã“ã‚Œã‚‰ã®è©³ç´°ã¯ã€ãƒã‚±ãƒƒãƒˆãŒãƒ›ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ç¢ºèªã§ãã¾ã™ã€‚

ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã®ä½¿ç”¨ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«ã€æ§‹æˆã®è©³ç´°ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã«å¤‰æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
## The value of the URL is fixed.
CLOUD_API_ENDPOINT = "https://api.cloud.zilliz.com"
API_KEY=""

# Configs for Zilliz Cloud cluster
CLUSTER_ENDPOINT=""
CLUSTER_ID="" # Zilliz Cloud cluster ID, like "in01-xxxxxxxxxxxxxxx"
COLLECTION_NAME="zero_to_hero"

# Configs for remote bucket
BUCKET_NAME=""
ACCESS_KEY=""
SECRET_KEY=""
```

</TabItem>

<TabItem value='java'>

```java
/**
 * The value of the URL is fixed.
 */
String CLOUD_API_ENDPOINT = "https://api.cloud.zilliz.com";
String API_KEY = "";

// Configs for Zilliz Cloud cluster
String CLUSTER_ENDPOINT = "";
String CLUSTER_ID = ""; // Zilliz Cloud cluster ID, like "in01-xxxxxxxxxxxxxxx"
String COLLECTION_NAME = "zero_to_hero";

// Configs for remote bucket
String BUCKET_NAME = "";
String ACCESS_KEY = "";
String SECRET_KEY = "";
```

</TabItem>
</Tabs>

## ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚¹ã‚­ãƒ¼ãƒã‚’è¨­å®šã™ã‚‹{#set-up-target-collection-schema}

ä¸Šè¨˜ã®å‡ºåŠ›ã«åŸºã¥ã„ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ã‚­ãƒ¼ãƒã‚’ä½œæˆã§ãã¾ã™ã€‚

æ¬¡ã®ãƒ‡ãƒ¢ã§ã¯ã€äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒã«æœ€åˆã®4ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ã€ä»–ã®4ã¤ã‚’å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>

<TabItem value='python'>

```python
from pymilvus import MilvusClient, DataType

# You need to work out a collection schema out of your dataset.
schema = MilvusClient.create_schema(
    auto_id=False,
    enable_dynamic_field=True
)

DIM = 512

schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True),
schema.add_field(field_name="bool", datatype=DataType.BOOL),
schema.add_field(field_name="int8", datatype=DataType.INT8),
schema.add_field(field_name="int16", datatype=DataType.INT16),
schema.add_field(field_name="int32", datatype=DataType.INT32),
schema.add_field(field_name="int64", datatype=DataType.INT64),
schema.add_field(field_name="float", datatype=DataType.FLOAT),
schema.add_field(field_name="double", datatype=DataType.DOUBLE),
schema.add_field(field_name="varchar", datatype=DataType.VARCHAR, max_length=512),
schema.add_field(field_name="json", datatype=DataType.JSON),
schema.add_field(field_name="array_str", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.VARCHAR, max_length=128)
schema.add_field(field_name="array_int", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.INT64)
schema.add_field(field_name="float_vector", datatype=DataType.FLOAT_VECTOR, dim=DIM),
schema.add_field(field_name="binary_vector", datatype=DataType.BINARY_VECTOR, dim=DIM),
schema.add_field(field_name="float16_vector", datatype=DataType.FLOAT16_VECTOR, dim=DIM),
# schema.add_field(field_name="bfloat16_vector", datatype=DataType.BFLOAT16_VECTOR, dim=DIM),
schema.add_field(field_name="sparse_vector", datatype=DataType.SPARSE_FLOAT_VECTOR)

schema.verify()

print(schema)
```

ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ¬¡ã®ã‚ˆã†ã«èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™

- ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:

    - `id`ã¯ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã™ã€‚

    - `float_vector`ã¯æµ®å‹•å°æ•°ç‚¹ãƒ™ã‚¯ãƒˆãƒ«å ´ã§ã™ã€‚

    - `binary_vector`ã¯ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«å ´ã§ã™ã€‚

    - `float 16_vector`ã¯ã€åŠç²¾åº¦ã®æµ®å‹•å°æ•°ç‚¹ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã™ã€‚

    - `sparse_vector`ã¯ç–ãƒ™ã‚¯ãƒˆãƒ«å ´ã§ã™ã€‚

    - æ®‹ã‚Šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ã‚¹ã‚«ãƒ©ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã™ã€‚

- `auto_id=Falseã®å ´åˆ`

    ã“ã‚ŒãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ã™ã€‚**True**ã«è¨­å®šã™ã‚‹ã¨ã€**BulkWriter**ã¯ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã‚ã¾ã›ã‚“ã€‚

- `Enable_Dynamicãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹`

    å€¤ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯**False**ã§ã™ã€‚ã“ã‚Œã‚’**True**ã«è¨­å®šã™ã‚‹ã¨ã€**BulkWriter**ã¯æœªå®šç¾©ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ã‚’ã‚­ãƒ¼ã¨å€¤ã®ãƒšã‚¢ã¨ã—ã¦å«ã‚ã€äºˆç´„æ¸ˆã¿ã®JSONãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰**$meta**ã«é…ç½®ã—ã¾ã™ã€‚

</TabItem>

<TabItem value='java'>

```java
import com.google.gson.Gson;
import com.google.gson.JsonObject;
import io.milvus.bulkwriter.BulkImport;
import io.milvus.bulkwriter.RemoteBulkWriter;
import io.milvus.bulkwriter.RemoteBulkWriterParam;
import io.milvus.bulkwriter.common.clientenum.BulkFileType;
import io.milvus.bulkwriter.common.clientenum.CloudStorage;
import io.milvus.bulkwriter.connect.S3ConnectParam;
import io.milvus.bulkwriter.connect.StorageConnectParam;
import io.milvus.bulkwriter.request.describe.MilvusDescribeImportRequest;
import io.milvus.bulkwriter.request.import_.MilvusImportRequest;
import io.milvus.bulkwriter.request.list.MilvusListImportJobsRequest;
import io.milvus.common.utils.Float16Utils;
import io.milvus.v2.client.ConnectConfig;
import io.milvus.v2.client.MilvusClientV2;
import io.milvus.v2.common.DataType;
import io.milvus.v2.service.collection.request.*;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.*;
import java.util.concurrent.TimeUnit;

private static final String STORAGE_ENDPOINT = CloudStorage.AWS.getEndpoint();
private static final String BUCKET_NAME = "a-bucket";
private static final String ACCESS_KEY = "access-key";
private static final String SECRET_KEY = "secret-key";

private static final Integer DIM = 512;
private static final Gson GSON_INSTANCE = new Gson();

private static CreateCollectionReq.CollectionSchema createSchema() {
    CreateCollectionReq.CollectionSchema schema = CreateCollectionReq.CollectionSchema.builder()
        .enableDynamicField(true)
        .build();
    schema.addField(AddFieldReq.builder()
            .fieldName("id")
            .dataType(io.milvus.v2.common.DataType.Int64)
            .isPrimaryKey(Boolean.TRUE)
            .autoID(false)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("bool")
            .dataType(DataType.Bool)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("int8")
            .dataType(DataType.Int8)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("int16")
            .dataType(DataType.Int16)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("int32")
            .dataType(DataType.Int32)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("int64")
            .dataType(DataType.Int64)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("float")
            .dataType(DataType.Float)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("double")
            .dataType(DataType.Double)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("varchar")
            .dataType(DataType.VarChar)
            .maxLength(512)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("json")
            .dataType(io.milvus.v2.common.DataType.JSON)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("array_int")
            .dataType(io.milvus.v2.common.DataType.Array)
            .maxCapacity(100)
            .elementType(io.milvus.v2.common.DataType.Int64)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("array_str")
            .dataType(io.milvus.v2.common.DataType.Array)
            .maxCapacity(100)
            .elementType(io.milvus.v2.common.DataType.VarChar)
            .maxLength(128)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("float_vector")
            .dataType(io.milvus.v2.common.DataType.FloatVector)
            .dimension(DIM)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("binary_vector")
            .dataType(io.milvus.v2.common.DataType.BinaryVector)
            .dimension(DIM)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("float16_vector")
            .dataType(io.milvus.v2.common.DataType.Float16Vector)
            .dimension(DIM)
            .build());
    schema.addField(AddFieldReq.builder()
            .fieldName("sparse_vector")
            .dataType(io.milvus.v2.common.DataType.SparseFloatVector)
            .build());
    
    return schema;
}
```

ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ã€

- [`id`]é …ç›®ã¯ãƒ—ãƒ©ã‚¤ãƒãƒªé …ç›®ã§ã€`with AutoID`ãŒ`false`ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«`id`é …ç›®ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

- float`_vector`ã€`binary_vector`ã€float`16_vector`ã€`sparse_vector`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã™ã€‚

- ã‚¹ã‚­ãƒ¼ãƒãŒ`withEnableDynamicField`ã«`true`ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ä»¥å¤–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

</TabItem>

</Tabs>

ã‚¹ã‚­ãƒ¼ãƒãŒè¨­å®šã•ã‚ŒãŸã‚‰ã€æ¬¡ã®ã‚ˆã†ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã§ãã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
from pymilvus import MilvusClient

# 1. Set up a Milvus client
client = MilvusClient(
    uri=CLUSTER_ENDPOINT,
    token=API_KEY
)

# 2. Set index parameters
index_params = MilvusClient.prepare_index_params()

index_params.add_index(
    field_name="float_vector",
    index_type="AUTOINDEX",
    metric_type="IP"
)

index_params.add_index(
    field_name="binary_vector",
    index_type="AUTOINDEX",
    metric_type="HAMMING"
)

index_params.add_index(
    field_name="float16_vector",
    index_type="AUTOINDEX",
    metric_type="IP"
)

index_params.add_index(
    field_name="sparse_vector",
    index_type="AUTOINDEX",
    metric_type="IP"
)

# 3. Create collection
client.create_collection(
    collection_name=COLLECTION_NAME,
    schema=schema,
    index_params=index_params
)
```

</TabItem>

<TabItem value='java'>

```java
import com.google.common.collect.Lists;
import io.milvus.v2.client.ConnectConfig;
import io.milvus.v2.client.MilvusClientV2;
import io.milvus.v2.common.IndexParam;
import io.milvus.v2.service.collection.request.CreateCollectionReq;
import java.util.List;

// 1. Set up a Milvus client
MilvusClientV2 milvusClient = new MilvusClientV2(ConnectConfig.builder()
        .uri(CLUSTER_ENDPOINT)
        .token(API_KEY)
        .build());

// 2. Set index parameters
IndexParam floatVectorIndex = IndexParam.builder()
        .fieldName("float_vector")
        .indexType(IndexParam.IndexType.AUTOINDEX)
        .metricType(IndexParam.MetricType.IP)
        .build();

IndexParam binaryVectorIndex = IndexParam.builder()
        .fieldName("binary_vector")
        .indexType(IndexParam.IndexType.AUTOINDEX)
        .metricType(IndexParam.MetricType.HAMMING)
        .build();

IndexParam float16VectorIndex = IndexParam.builder()
        .fieldName("float16_vector")
        .indexType(IndexParam.IndexType.AUTOINDEX)
        .metricType(IndexParam.MetricType.IP)
        .build();

IndexParam sparseVectorIndex = IndexParam.builder()
        .fieldName("sparse_vector")
        .indexType(IndexParam.IndexType.AUTOINDEX)
        .metricType(IndexParam.MetricType.IP)
        .build();

List<IndexParam> indexParamList = Lists.newArrayList(
        floatVectorIndex,
        binaryVectorIndex,
        float16VectorIndex,
        sparseVectorIndex
);

// 3. Create collection
CreateCollectionReq.CollectionSchema schema = createSchema();
CreateCollectionReq request = CreateCollectionReq.builder()
        .collectionName(COLLECTION_NAME)
        .collectionSchema(schema)
        .indexParams(indexParamList)
        .build();
milvusClient.createCollection(request);
```

</TabItem>
</Tabs>

## ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹{#prepare-source-data}

**BulkWriter**ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’JSONã€Parquetã€ã¾ãŸã¯NumPyãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚RemoteBulkWriterã‚’ä½œæˆã—**ã€**ã“ã‚Œã‚‰ã®å½¢å¼ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãæ›ãˆã¾ã™ã€‚

### RemoteBulkWriterã®ä½œæˆ{#create-remotebulkwriter}

ã‚¹ã‚­ãƒ¼ãƒãŒæº–å‚™ã§ããŸã‚‰ã€ã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦RemoteBulkWriterã‚’ä½œæˆã§ãã¾ã™ã€‚**RemoteBulkWriter**ã¯ã€**ãƒªãƒ¢ãƒ¼ãƒˆ**ãƒã‚±ãƒƒãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ¨©é™ã‚’è¦æ±‚ã—ã¾ã™ã€‚ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®æ¥ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’**ConnectParam**ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§è¨­å®šã—ã€RemoteBulkWriterã§å‚ç…§ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾**ã™**ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

<Tabs groupId="python" defaultValue='python' values={[{"label":"AWS S3/GCS","value":"python"},{"label":"Microsoft Azure","value":"python_1"}]}>
<TabItem value='python'>

```python

from pymilvus.bulk_writer import RemoteBulkWriter, BulkFileType
# Use `from pymilvus import RemoteBulkWriter, BulkFileType`
# if your pymilvus version is earlier than 2.4.2 

# Connections parameters to access the remote bucket
conn = RemoteBulkWriter.S3ConnectParam(
    endpoint="s3.amazonaws.com", # Use "storage.googleapis.com" for Google Cloud Storage
    access_key=ACCESS_KEY,
    secret_key=SECRET_KEY,
    bucket_name=BUCKET_NAME, 
    secure=True
)

```

</TabItem>
<TabItem value='python_1'>

```python
# Third-party constants
AZURE_CONNECT_STRING = ""

conn = RemoteBulkWriter.AzureConnectParam(
    conn_str=AZURE_CONNECT_STRING,
    container_name=BUCKET_NAME
)

# or

# Thrid-party constants
AZURE_ACCOUNT_URL = ""
AZURE_CREDENTIAL = ""

conn = RemoteBulkWriter.AzureConnectParam(
    account_url=AZURE_ACCOUNT_URL,
    credential=AZURE_CREDENTIAL,
    container_name=BUCKET_NAME
)
```

</TabItem>
</Tabs>
</TabItem>

<TabItem value='java'>

<Tabs groupId="java" defaultValue='java' values={[{"label":"AWS S3/GCS","value":"java"},{"label":"Microsoft Azure","value":"java_1"}]}>
<TabItem value='java'>

```java

import io.milvus.bulkwriter.connect.S3ConnectParam;
import io.milvus.bulkwriter.connect.StorageConnectParam;

// Create a remote bucket writer.
StorageConnectParam storageConnectParam = S3ConnectParam.newBuilder()
        .withEndpoint("s3.amazonaws.com") // Use "storage.googleapis.com" for Google Cloud Storage
        .withBucketName(BUCKET_NAME)
        .withAccessKey(ACCESS_KEY)
        .withSecretKey(SECRET_KEY)
        .build();

```

</TabItem>
<TabItem value='java_1'>

```java
import io.milvus.bulkwriter.connect.AzureConnectParam;
import io.milvus.bulkwriter.connect.StorageConnectParam;

String AZURE_CONNECT_STRING = ""
String AZURE_CONTAINER = ""

StorageConnectParam storageConnectParam = AzureConnectParam.newBuilder()
        .withConnStr(AZURE_CONNECT_STRING)
        .withContainerName(AZURE_CONTAINER)
        .build()
```

</TabItem>
</Tabs>
</TabItem>
</Tabs>

<Admonition type="info" icon="ğŸ“˜" title="ãƒãƒ¼ãƒˆ">

<p>ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ<strong>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼</strong>ã¯ã€ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹URIã‚’å‚ç…§ã—ã¾ã™ã€‚</p>
<p>S 3äº’æ›ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹ã®å ´åˆã€ä½¿ç”¨å¯èƒ½ãªURIã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚</p>
<ul>
<li><p><code>s3.amazonaws.com</code>AWSã®å ´åˆ</p></li>
<li><p><code>storage.googleapis.com</code>ï¼ˆGCSã®ï¼‰</p></li>
</ul>
<p>Azure BLOBã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒŠãƒ¼ã®å ´åˆã¯ã€æ¬¡ã®ã‚ˆã†<a href="https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage#view-account-access-keys">ãªæœ‰åŠ¹ãªæ¥ç¶šæ–‡å­—åˆ—</a>ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p>
<p><code>DefaultEndpointsProtocol=https;ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå=&lt;ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå&gt;;ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼=&lt;ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼&gt;;ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹=core.windows.net</code></p>

</Admonition>

æ¬¡ã«ã€**RemoteBulkWriter**ã®æ¥ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ¬¡ã®ã‚ˆã†ã«å‚ç…§ã§ãã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>

<TabItem value='python'>

```python
writer = RemoteBulkWriter(
    schema=schema, # Target collection schema
    remote_path="/", # Output directory relative to the remote bucket root
    segment_size=1024*1024*1024, # Maximum segment size when segmenting the raw data
    connect_param=conn, # Connection parameters defined above
    file_type=BulkFileType.PARQUET # Type of the generated file.
)

# Possible file types:
# - BulkFileType.JSON_RB, 
# - BulkFileType.NPY, and 
# - BulkFileType.PARQUET
```

ä¸Šè¨˜ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã¯JSONå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€æŒ‡å®šã•ã‚ŒãŸãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

- `ãƒªãƒ¢ãƒ¼ãƒˆãƒ‘ã‚¹ã®è¨­å®š`

    ã“ã‚Œã«ã‚ˆã‚Šã€ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆå†…ã®ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‘ã‚¹ãŒæ±ºå®šã•ã‚Œã¾ã™ã€‚

    ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’`"/"`ã«è¨­å®šã™ã‚‹ã¨ã€**RemoteBulkWriter**ãŒãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¾ã™ã€‚ä»–ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚

- `file_type=ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—`

    ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’æ±ºå®šã—ã¾ã™ã€‚å¯èƒ½ãªå€¤ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

    - **BulkFileType. JSON_RBãƒ•ã‚¡ã‚¤ãƒ«**

    - **ãƒãƒ«ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—. PARQUET**

    - **ä¸€æ‹¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—. NPY**

- `ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚µã‚¤ã‚º=1024*1024*102 4`

    ã“ã‚Œã«ã‚ˆã‚Šã€**BulkWriter**ãŒç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹ã‹ã©ã†ã‹ãŒæ±ºã¾ã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯1024 MBï¼ˆ1024*1024*1024ï¼‰ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤šæ•°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€**sement_size**ã‚’é©åˆ‡ãªå€¤ã«è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

</TabItem>

<TabItem value='java'>

```java
import io.milvus.bulkwriter.RemoteBulkWriter;
import io.milvus.bulkwriter.RemoteBulkWriterParam;
import io.milvus.bulkwriter.common.clientenum.BulkFileType;

RemoteBulkWriterParam remoteBulkWriterParam = RemoteBulkWriterParam.newBuilder()
        .withCollectionSchema(schema)
        .withRemotePath("/")
        .withChunkSize(1024 * 1024 * 1024)
        .withConnectParam(storageConnectParam)
        .withFileType(BulkFileType.PARQUET)
        .build();
        
@SuppressWarnings("resource")
RemoteBulkWriter remoteBulkWriter = new RemoteBulkWriter(remoteBulkWriterParam);

// Possible file types:
// - BulkFileType.PARQUET
```

ä¸Šè¨˜ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã¯ã€Parquetå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€æŒ‡å®šã•ã‚ŒãŸãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

- `withRemotePath("/")ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚`

    ã“ã‚Œã«ã‚ˆã‚Šã€ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆå†…ã®ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‘ã‚¹ãŒæ±ºå®šã•ã‚Œã¾ã™ã€‚

    ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’`"/"`ã«è¨­å®šã™ã‚‹ã¨ã€**RemoteBulkWriter**ãŒãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¾ã™ã€‚ä»–ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚

- `ãƒ‘ãƒ¼ã‚±ãƒƒãƒˆ(BulkFileType. PARQUET)`

    ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¨®é¡ã‚’æ±ºå®šã—ã¾ã™ã€‚ç¾åœ¨ã€**PARQUET**ã®ã¿ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚

- `with ChunkSize(1024*1024*102 4)ã®ã‚µã‚¤ã‚ºã§ã™ã€‚`

    ã“ã‚Œã«ã‚ˆã‚Šã€**BulkWriter**ãŒç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹ã‹ã©ã†ã‹ãŒæ±ºã¾ã‚Šã¾ã™ã€‚å€¤ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1024 MB(1024*1024*1024)ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤šæ•°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€withChunkSizeã‚’é©åˆ‡ãªå€¤ã«è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ**åŒ–**ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

</TabItem>

</Tabs>

### ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹{#use-the-writer}

ãƒ©ã‚¤ã‚¿ãƒ¼ã«ã¯2ã¤ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™ã€‚1ã¤ã¯ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¡Œã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã€ã‚‚ã†1ã¤ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ¢ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

æ¬¡ã®ã‚ˆã†ã«ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¡Œã‚’è¿½åŠ ã§ãã¾ã™:

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
import random, string, json
import numpy as np
import tensorflow as tf

def generate_random_str(length=5):
    letters = string.ascii_uppercase
    digits = string.digits
    
    return ''.join(random.choices(letters + digits, k=length))

# optional input for binary vector:
# 1. list of int such as [1, 0, 1, 1, 0, 0, 1, 0]
# 2. numpy array of uint8
def gen_binary_vector(to_numpy_arr):
    raw_vector = [random.randint(0, 1) for i in range(DIM)]
    if to_numpy_arr:
        return np.packbits(raw_vector, axis=-1)
    return raw_vector

# optional input for float vector:
# 1. list of float such as [0.56, 1.859, 6.55, 9.45]
# 2. numpy array of float32
def gen_float_vector(to_numpy_arr):
    raw_vector = [random.random() for _ in range(DIM)]
    if to_numpy_arr:
        return np.array(raw_vector, dtype="float32")
    return raw_vector

# # optional input for bfloat16 vector:
# # 1. list of float such as [0.56, 1.859, 6.55, 9.45]
# # 2. numpy array of bfloat16
# def gen_bf16_vector(to_numpy_arr):
#     raw_vector = [random.random() for _ in range(DIM)]
#     if to_numpy_arr:
#         return tf.cast(raw_vector, dtype=tf.bfloat16).numpy()
#     return raw_vector

# optional input for float16 vector:
# 1. list of float such as [0.56, 1.859, 6.55, 9.45]
# 2. numpy array of float16
def gen_fp16_vector(to_numpy_arr):
    raw_vector = [random.random() for _ in range(DIM)]
    if to_numpy_arr:
        return np.array(raw_vector, dtype=np.float16)
    return raw_vector

# optional input for sparse vector:
# only accepts dict like {2: 13.23, 45: 0.54} or {"indices": [1, 2], "values": [0.1, 0.2]}
# note: no need to sort the keys
def gen_sparse_vector(pair_dict: bool):
    raw_vector = {}
    dim = random.randint(2, 20)
    if pair_dict:
        raw_vector["indices"] = [i for i in range(dim)]
        raw_vector["values"] = [random.random() for _ in range(dim)]
    else:
        for i in range(dim):
            raw_vector[i] = random.random()
    return raw_vector

for i in range(2000):
    writer.append_row({
        "id": np.int64(i),
        "bool": True if i % 3 == 0 else False,
        "int8": np.int8(i%128),
        "int16": np.int16(i%1000),
        "int32": np.int32(i%100000),
        "int64": np.int64(i),
        "float": np.float32(i/3),
        "double": np.float64(i/7),
        "varchar": f"varchar_{i}",
        "json": json.dumps({"dummy": i, "ok": f"name_{i}"}),
        "array_str": np.array([f"str_{k}" for k in range(5)], np.dtype("str")),
        "array_int": np.array([k for k in range(10)], np.dtype("int64")),
        "float_vector": gen_float_vector(True),
        "binary_vector": gen_binary_vector(True),
        "float16_vector": gen_fp16_vector(True),
        # "bfloat16_vector": gen_bf16_vector(True),
        "sparse_vector": gen_sparse_vector(True),
        f"dynamic_{i}": i,
    })
    if (i+1)%1000 == 0:
        writer.commit()
        print('committed')

print(writer.batch_files)
```

</TabItem>

<TabItem value='java'>

```java
import com.google.gson.JsonObject;
import io.milvus.bulkwriter.RemoteBulkWriter;
import io.milvus.bulkwriter.RemoteBulkWriterParam;
import io.milvus.bulkwriter.common.clientenum.BulkFileType;
import io.milvus.bulkwriter.connect.S3ConnectParam;
import io.milvus.bulkwriter.connect.StorageConnectParam;
import io.milvus.common.utils.Float16Utils;
import io.milvus.v2.service.collection.request.CreateCollectionReq;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;
import java.util.SortedMap;
import java.util.TreeMap;

private static byte[] genBinaryVector() {
    Random ran = new Random();
    int byteCount = DIM / 8;
    ByteBuffer vector = ByteBuffer.allocate(byteCount);
    for (int i = 0; i < byteCount; ++i) {
        vector.put((byte) ran.nextInt(Byte.MAX_VALUE));
    }
    return vector.array();
}

private static List<Float> genFloatVector() {
    Random ran = new Random();
    List<Float> vector = new ArrayList<>();
    for (int i = 0; i < DIM; ++i) {
        vector.add(ran.nextFloat());
    }
    return vector;
}

private static byte[] genFloat16Vector() {
    List<Float> originalVector = genFloatVector();
    return Float16Utils.f32VectorToFp16Buffer(originalVector).array();
}

private static SortedMap<Long, Float> genSparseVector() {
    Random ran = new Random();
    SortedMap<Long, Float> sparse = new TreeMap<>();
    int dim = ran.nextInt(18) + 2; // [2, 20)
    for (int i = 0; i < dim; ++i) {
        sparse.put((long)ran.nextInt(1000000), ran.nextFloat());
    }
    return sparse;
}

private static List<String> genStringArray(int length) {
    List<String> arr = new ArrayList<>();
    for (int i = 0; i < length; i++) {
        arr.add("str_" + i);
    }
    return arr;
}

private static List<Long> genIntArray(int length) {
    List<Long> arr = new ArrayList<>();
    for (long i = 0; i < length; i++) {
        arr.add(i);
    }
    return arr;
}

private static RemoteBulkWriter createRemoteBulkWriter(CreateCollectionReq.CollectionSchema collectionSchema) throws IOException {
    StorageConnectParam connectParam = S3ConnectParam.newBuilder()
            .withEndpoint(STORAGE_ENDPOINT)
            .withBucketName(BUCKET_NAME)
            .withAccessKey(ACCESS_KEY)
            .withSecretKey(SECRET_KEY)
            .build();
    RemoteBulkWriterParam bulkWriterParam = RemoteBulkWriterParam.newBuilder()
            .withCollectionSchema(collectionSchema)
            .withRemotePath("/")
            .withChunkSize(1024 * 1024 * 1024)
            .withConnectParam(connectParam)
            .withFileType(BulkFileType.PARQUET)
            .build();
    return new RemoteBulkWriter(bulkWriterParam);
}

private static List<List<String>> uploadData() throws Exception {
    CreateCollectionReq.CollectionSchema collectionSchema = createSchema();
    try (RemoteBulkWriter remoteBulkWriter = createRemoteBulkWriter(collectionSchema)) {
        for (int i = 0; i < 2000; ++i) {
            JsonObject rowObject = new JsonObject();

            rowObject.addProperty("id", i);
            rowObject.addProperty("bool", i % 3 == 0);
            rowObject.addProperty("int8", i % 128);
            rowObject.addProperty("int16", i % 1000);
            rowObject.addProperty("int32", i % 100000);
            rowObject.addProperty("int64", i);
            rowObject.addProperty("float", i / 3);
            rowObject.addProperty("double", i / 7);
            rowObject.addProperty("varchar", "varchar_" + i);
            rowObject.addProperty("json", String.format("{\"dummy\": %s, \"ok\": \"name_%s\"}", i, i));
            rowObject.add("array_str", GSON_INSTANCE.toJsonTree(genStringArray(5)));
            rowObject.add("array_int", GSON_INSTANCE.toJsonTree(genIntArray(10)));
            rowObject.add("float_vector", GSON_INSTANCE.toJsonTree(genFloatVector()));
            rowObject.add("binary_vector", GSON_INSTANCE.toJsonTree(genBinaryVector()));
            rowObject.add("float16_vector", GSON_INSTANCE.toJsonTree(genFloat16Vector()));
            rowObject.add("sparse_vector", GSON_INSTANCE.toJsonTree(genSparseVector()));
            rowObject.addProperty("dynamic", "dynamic_" + i);

            remoteBulkWriter.appendRow(rowObject);

            if ((i+1)%1000 == 0) {
                remoteBulkWriter.commit(false);
            }
        }

        List<List<String>> batchFiles = remoteBulkWriter.getBatchFiles();
        System.out.println(batchFiles);
        return batchFiles;
    } catch (Exception e) {
        throw e;
    }
}

public static void main(String[] args) throws Exception {
    List<List<String>> batchFiles = uploadData();
}
```

</TabItem>
</Tabs>

ãƒ©ã‚¤ã‚¿ãƒ¼ã®**append_row()**ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€è¡Œã®è¾æ›¸ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚

è¡Œãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã«ã¯ã€ã™ã¹ã¦ã®ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚­ãƒ¼ã¨ã—ã¦å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æœªå®šç¾©ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚è©³ç´°ã¯ã€ã€Œ[BulkWriterã‚’ä½¿ã†](./use-bulkwriter)ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

**BulkWriter**ã¯ã€**commit()**ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ãŸå¾Œã«ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
writer.commit()
```

</TabItem>

<TabItem value='java'>

```java
remoteBulkWriter.commit(false);
```

</TabItem>
</Tabs>

ä»Šã¾ã§ã€**BulkWriter**ã¯æŒ‡å®šã•ã‚ŒãŸãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆã«ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ã„ã¾ã™ã€‚

ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€ãƒ©ã‚¤ã‚¿ãƒ¼ã®**data_path**ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å°åˆ·ã—ã¦å®Ÿéš›ã®å‡ºåŠ›ãƒ‘ã‚¹ã‚’å–å¾—ã§ãã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
print(writer.data_path)

# /5868ba87-743e-4d9e-8fa6-e07b39229425
```

</TabItem>

<TabItem value='java'>

```java
import java.util.List;

List<List<String>> batchFiles = remoteBulkWriter.getBatchFiles();
System.out.println(batchFiles);

// [["/5868ba87-743e-4d9e-8fa6-e07b39229425/1.parquet"]]
```

</TabItem>
</Tabs>

<Admonition type="info" icon="ğŸ“˜" title="ãƒãƒ¼ãƒˆ">

<p><strong>BulkWriter</strong>ã¯UUIDã‚’ç”Ÿæˆã—ã€æŒ‡å®šã•ã‚ŒãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«UUIDã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã€ç”Ÿæˆã•ã‚ŒãŸã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¾ã™ã€‚</p>

</Admonition>

è©³ç´°ã¯ã€ã€Œ[BulkWriterã‚’ä½¿ã†](./use-bulkwriter)ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## æº–å‚™ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹{#import-prepared-data}

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‰ã«ã€æº–å‚™ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒç›®çš„ã®ãƒã‚±ãƒƒãƒˆã«ã™ã§ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

### ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é–‹å§‹{#start-importing}

æº–å‚™ã—ãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚ˆã†ã«**bulk_import()**é–¢æ•°ã‚’å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
from pymilvus.bulk_writer import bulk_import

# Publicly accessible URL for the prepared data in the remote bucket
object_url = "s3://{0}/{1}/".format(BUCKET_NAME, str(writer.data_path)[1:])
# Change `s3` to `gs` for Google Cloud Storage

resp = bulk_import(
    api_key=API_KEY,
    url=CLOUD_API_ENDPOINT,
    cluster_id=CLUSTER_ID,
    collection_name=COLLECTION_NAME,
    object_url=object_url,
    access_key=ACCESS_KEY,
    secret_key=SECRET_KEY
)

job_id = resp.json()['data']['jobId']
print(job_id)

# job-0103f039ccdq9aip1xd4rf
```

</TabItem>

<TabItem value='java'>

```java
import io.milvus.bulkwriter.request.import_.CloudImportRequest;
import io.milvus.bulkwriter.BulkImport;

// Insert the data into the collection
String prefix = batchFiles.get(0).get(0).split("/")[0];
String OBJECT_URL = String.format("s3://%s/%s/", BUCKET_NAME, prefix);

CloudImportRequest cloudImportRequest = CloudImportRequest.builder()
        .apiKey(API_KEY)
        .clusterId(CLUSTER_ID)
        .collectionName(COLLECTION_NAME)
        .objectUrl(OBJECT_URL)
        .accessKey(ACCESS_KEY)
        .secretKey(SECRET_KEY)
        .build();
String bulkImportResult = BulkImport.bulkImport(CLOUD_API_ENDPOINT, cloudImportRequest);

JsonObject bulkImportObject = new Gson().fromJson(bulkImportResult, JsonObject.class);
String jobId = bulkImportObject.getAsJsonObject("data").get("jobId").getAsString();
System.out.println(jobId);
// job-0103f039ccdq9aip1xd4rf
```

</TabItem>
</Tabs>

<Admonition type="info" icon="ğŸ“˜" title="ãƒãƒ¼ãƒˆ">

<p>object<strong>_urlã¯</strong>ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®æœ‰åŠ¹ãªURLã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æä¾›ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã§ã¯ã€<strong>format()</strong>ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒã‚±ãƒƒãƒˆåã¨ãƒ©ã‚¤ã‚¿ãƒ¼ã«ã‚ˆã£ã¦è¿”ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æœ‰åŠ¹ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆURLã‚’ä½œæˆã—ã¾ã™ã€‚</p>
<p>ãƒ‡ãƒ¼ã‚¿ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒAWSã«ã‚ˆã£ã¦ãƒ›ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆURLã¯<strong>s 3://remote-bucket/file-path</strong>ã«ä¼¼ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ©ã‚¤ã‚¿ãƒ¼ã«ã‚ˆã£ã¦è¿”ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã«é©ç”¨å¯èƒ½ãªURIã«ã¤ã„ã¦ã¯ã€ã€Œ<a href="./data-import-storage-options">ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚ªãƒ—ã‚·ãƒ§ãƒ³</a>ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚</p>

</Admonition>

### ã‚¿ã‚¹ã‚¯ã®é€²æ—ã‚’ç¢ºèªã™ã‚‹{#check-task-progress}

æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã¯ã€5ç§’ã”ã¨ã«ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®é€²è¡ŒçŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€é€²è¡ŒçŠ¶æ³ã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã§å‡ºåŠ›ã—ã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
import time
from pymilvus import get_import_progress

job_id = res.json()['data']['jobId']

res = get_import_progress(
    api_key=API_KEY,
    url=CLOUD_API_ENDPOINT,
    cluster_id=CLUSTER_ID,  # Zilliz Cloud cluster ID, like "in01-xxxxxxxxxxxxxxx"
    job_id=job_id,
)

print(res.json()["data"]["progress"])

# check the bulk-import progress
while res.json()["data"]["progress"] < 100:
    time.sleep(5)

    res = get_import_progress(
        # highlight-next-line
        url=CLOUD_API_ENDPOINT,
        api_key=API_KEY,
        job_id=job_id,
        cluster_id=CLUSTER_ID
    )
    
    print(res.json()["data"]["progress"])

# 0   -- import progress 0%
# 49  -- import progress 49%
# 100 -- import finished
```

</TabItem>

<TabItem value='java'>

```java
while (true) {
    System.out.println("Wait 5 second to check bulkInsert job state...");
    TimeUnit.SECONDS.sleep(5);
    
    CloudDescribeImportRequest request = CloudDescribeImportRequest.builder()
        .apiKey(API_KEY)
        .clusterId(CLUSTER_ID)
        .jobId(jobId)
        .build();
    String getImportProgressResult = BulkImport.getImportProgress(CLOUD_API_ENDPOINT, request);
    JsonObject getImportProgressObject = GSON_INSTANCE.fromJson(getImportProgressResult, JsonObject.class);
    String importProgressState = getImportProgressObject.getAsJsonObject("data").get("state").getAsString();
    String progress = getImportProgressObject.getAsJsonObject("data").get("progress").getAsString();

    if ("Failed".equals(importProgressState)) {
        String reason = getImportProgressObject.getAsJsonObject("data").get("reason").getAsString();
        System.out.printf("The job %s failed, reason: %s%n", jobId, reason);
        break;
    } else if ("Completed".equals(importProgressState)) {
        System.out.printf("The job %s completed%n", jobId);
        break;
    } else {
        System.out.printf("The job %s is running, state:%s progress:%s%n", jobId, importProgressState, progress);
    }
}

// The job job-01f36d8fd67u94avjfnxi0 is running, state:Importing progress:0
// The job job-01f36d8fd67u94avjfnxi0 is running, state:Importing progress:49
// The job 0f7fe853-d93e-4681-99f2-4719c63585cc completed.
```

</TabItem>
</Tabs>

<Admonition type="info" icon="ğŸ“˜" title="ãƒãƒ¼ãƒˆ">

<p>get<em>i mport</em>gress<strong>(</strong>)å†…ã®<strong>urlã‚’</strong>ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒ©ã‚¦ãƒ‰ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«å¯¾å¿œã™ã‚‹ã‚‚ã®ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚</p>

</Admonition>

ã™ã¹ã¦ã®ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¸ãƒ§ãƒ–ã‚’æ¬¡ã®ã‚ˆã†ã«ä¸€è¦§è¡¨ç¤ºã§ãã¾ã™ã€‚

<Tabs groupId="code" defaultValue='python' values={[{"label":"Python","value":"python"},{"label":"Java","value":"java"}]}>
<TabItem value='python'>

```python
from pymilvus import list_import_jobs

res = list_import_jobs(
    api_key=API_KEY,
    # highlight-next-line
    url=CLOUD_API_ENDPOINT,
    cluster_id=CLUSTER_ID  # Zilliz Cloud cluster ID, like "in01-xxxxxxxxxxxxxxx"
)

print(res.json())

# {
#     "code": 0,
#     "data": {
#         "records": [
#             {
#                 "collectionName": "zero_to_hero",
#                 "jobId": "job-01f36d8fd67u94avjfnxi0",
#                 "state": "Completed"
#             }
#         ],
#         "count": 1,
#         "currentPage": 1,
#         "pageSize": 10
#     }
# }
```

</TabItem>

<TabItem value='java'>

```java
CloudListImportJobsRequest listImportJobsRequest = CloudListImportJobsRequest.builder()
        .apiKey(API_KEY)
        .clusterId(CLUSTER_ID) // Zilliz Cloud cluster ID, like "in01-xxxxxxxxxxxxxxx"
        .build();
String listImportJobsResult = BulkImport.listImportJobs(CLOUD_API_ENDPOINT, listImportJobsRequest);
System.out.println(listImportJobsResult);
```

</TabItem>
</Tabs>

## ã¾ã¨ã‚{#recaps}

ã“ã®ã‚³ãƒ¼ã‚¹ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å…¨éç¨‹ã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã«ã¾ã¨ã‚ã‚‹ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã„ãã¤ã‹ç´¹ä»‹ã—ã¾ã™ã€‚

- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ã‚­ãƒ¼ãƒã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã¦ãã ã•ã„ã€‚

- BulkWriterã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ**ã¯**ã€ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

    - å„è¡Œã«è¿½åŠ ã™ã‚‹ã‚­ãƒ¼ã¨ã—ã¦ã€ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã™ã¹ã¦å«ã‚ã¾ã™ã€‚å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€é©ç”¨å¯èƒ½ãªæœªå®šç¾©ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å«ã‚ã¾ã™ã€‚

    - ã™ã¹ã¦ã®è¡Œã‚’è¿½åŠ ã—ãŸå¾Œã«**commit()**ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚

- bulk_import**()**ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€æº–å‚™ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒ›ã‚¹ãƒˆã™ã‚‹ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ã€ãƒ©ã‚¤ã‚¿ãƒ¼ãŒè¿”ã™ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’é€£çµã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆURLã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

